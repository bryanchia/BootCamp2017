{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) \n",
    "\n",
    "Gradient Method: The gradient points in the dierction of greatest ascent. We move opposite to that to minimize the algorithm and take a step towards that direction.\n",
    "\n",
    "Newton's Method: It is a special case of the gradient method where alpha is equal to the inverse of the Hessian. \n",
    "\n",
    "BFGS Method: Start by solving for the Hessian, but later uses an approximation of the Hessian without actually computing it in order to save time on computation. It assumes that the gradient of the approximation is close to the approximation of gradient of f at $x_k$ and $x_{k+1}$\n",
    "\n",
    "Conjugate Gradient Method: Move along Q-conjugate directions, which are orthorgonal to the inner product. \n",
    "\n",
    "(ii)\n",
    "\n",
    "Gradient Method: If objective function is differentiable. \n",
    "\n",
    "Newton's Method: If the dimension of the problem is nto too large and the Hessian is positive definite and can be computed easily. If $n$ is large, it can be unstable to compute. \n",
    "\n",
    "Conjugate Gradient: Method of choice if $n$ is large and $m<n^2$. This will be faster than direct methods. \n",
    "\n",
    "(iii) & (iv)\n",
    "\n",
    "Gradient Method: Converges slowly but costs less per iteration. \n",
    "\n",
    "Newton's Method: converges quickly but costs more per iteration. \n",
    "\n",
    "BFGS Method: Storing approximation can be costly. Less costly per iteration than the Newton's method. \n",
    "\n",
    "Conjugate Gradient Method: If n is large, running the algorithm is not advantageous becuase the approximation is good enough after fewer steps. It may be expensive to carry out all n steps. Better conditioned matrices gives faster convergence. \n",
    "\n",
    "Converges more slowly and costing less per iteration than Newton's Methods but converges more rapidly and costing more per iteration than the steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.6\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfunc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e4e06b91b7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mmaxiters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton_mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Root ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m', Iterations ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m',Success ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfunc' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sy\n",
    "from scipy.misc import derivative\n",
    "import quantecon as qe\n",
    "import sympy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp\n",
    "import numpy.linalg as la\n",
    "from autograd import jacobian\n",
    "\n",
    "def get_jacob(func, x0):\n",
    "    jacobian_f = jacobian(func)\n",
    "    jac = jacobian_f(anp.array(x0))\n",
    "    return jac\n",
    "\n",
    "\n",
    "def newton_mv(func, x0, maxiters, alpha, dfunc = None,  epsilon = 0.01):\n",
    "    \n",
    "    x1 = np.ones(len(x0))\n",
    "    y = np.zeros(len(x0))\n",
    "    diff = 15 \n",
    "    success = \"False\"  \n",
    "    \n",
    "    for k in range(int(maxiters)):\n",
    "        \n",
    "        if np.all(dfunc(x0)) == 0:\n",
    "            x1 = 0\n",
    "            success = \"Cannot run Newton's Method\"\n",
    "            break\n",
    "        \n",
    "        else: \n",
    "            jac = get_jacob(func, x0)\n",
    "            y = la.solve(jac, func(x0))\n",
    "            x1 = x0 - alpha * y\n",
    "            diff = la.norm(x1 - x0)\n",
    "            if ( diff < epsilon ):\n",
    "                success = \"True\"\n",
    "                break\n",
    "            x0 = x1\n",
    "        \n",
    "    return x1, k, success\n",
    "\n",
    "x0 = np.array((1.0,1.0))\n",
    "maxiters = int(1000)\n",
    "alpha = 0.4\n",
    "x1, k, success = newton_mv(f5, x0, maxiters, alpha, dfunc)\n",
    "\n",
    "print('Root =', x1,', Iterations =', k, ',Success =', success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.10\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{1} = x_0 - D^2f(x_0)^{-1}Df(x_0)^T $$\n",
    "$$x_{1} = x_0 - 2Q^{-1}(\\frac{1}{2}Qx_0-b^T) $$\n",
    "$$x_{1} = 2Q^{-1}b^T $$\n",
    "$$x_{2} = 2Q^{-1}b^T - 2Q^{-1}(\\frac{1}{2}Q(2Q^{-1}b^T)-b^T) = 2Q^{-1}b^T = x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f5 = lambda x: anp.array([3 * x[0] **2, 5 * x[0] ** 3 + 3 *x[1] ])\n",
    "\n",
    "def get_jacob(func, x0):\n",
    "    jacobian_f = jacobian(func)\n",
    "    jac = jacobian_f(anp.array(x0))\n",
    "    return jac\n",
    "\n",
    "print((get_jacob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
